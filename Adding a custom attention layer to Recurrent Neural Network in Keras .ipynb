{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661df1ec",
   "metadata": {},
   "source": [
    "# The Import Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1a5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3ed77",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b426793",
   "metadata": {},
   "source": [
    "The following function generates a sequence of n Fibonacci number. If Sacale_data is set to True, then it would also use the MinMaxSaclar from scikit-learn to sacale the values between 0 and 1. Let's see its output for n=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab9263",
   "metadata": {},
   "source": [
    "Next, we need a function get_fib_XY() that reformats the sequence into training examples and target values to be used by the Keras input layer. When given time_steps as a parameter, get_fib_XY() constructs each row of the dataset with time_steps number of columns. This function not only constructs the training set and test set from the Fibonacci sequence but also shuffles the training examples and reshapes them to the required TensorFlow format, i.e., total_samples x time_steps x features. Also, the function returns the scaler object that scales the values if scale_data is set to True.\n",
    "\n",
    "Let’s generate a small training set to see what it looks like. We have set time_steps=3 and total_fib_numbers=12, with approximately 70% of the examples going toward the test points. Note the training and test examples have been shuffled by the permutation() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63972bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.  5.  8. 13. 21. 34. 55. 89.]\n"
     ]
    }
   ],
   "source": [
    "def get_fib_seq(n, scale_data=True):\n",
    "    \n",
    "    # Get the Fibonacci sequence\n",
    "    seq = np.zeros(n)\n",
    "    fib_n1 = 0.0\n",
    "    fib_n = 1.0 \n",
    "    for i in range(n):\n",
    "            seq[i] = fib_n1 + fib_n\n",
    "            fib_n1 = fib_n\n",
    "            fib_n = seq[i] \n",
    "    scaler = []\n",
    "    if scale_data:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        seq = np.reshape(seq, (n, 1))\n",
    "        seq = scaler.fit_transform(seq).flatten()        \n",
    "    return seq, scaler\n",
    " \n",
    "fib_seq = get_fib_seq(10, False)[0]\n",
    "print(fib_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501be8c",
   "metadata": {},
   "source": [
    "Next, we need a function get_fib_XY() that reformats the sequence into training examples and target values to be used by the Keras input layer. When given time_steps as a parameter, get_fib_XY() constructs each row of the dataset with time_steps number of columns. This function not only constructs the training set and test set from the Fibonacci sequence but also shuffles the training examples and reshapes them to the required TensorFlow format, i.e., total_samples x time_steps x features. Also, the function returns the scaler object that scales the values if scale_data is set to True.\n",
    "\n",
    "Let’s generate a small training set to see what it looks like. We have set time_steps=3 and total_fib_numbers=12, with approximately 70% of the examples going toward the test points. Note the training and test examples have been shuffled by the permutation() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f13104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX =  [[[ 8.]\n",
      "  [13.]\n",
      "  [21.]]\n",
      "\n",
      " [[ 5.]\n",
      "  [ 8.]\n",
      "  [13.]]\n",
      "\n",
      " [[ 2.]\n",
      "  [ 3.]\n",
      "  [ 5.]]\n",
      "\n",
      " [[13.]\n",
      "  [21.]\n",
      "  [34.]]\n",
      "\n",
      " [[21.]\n",
      "  [34.]\n",
      "  [55.]]\n",
      "\n",
      " [[34.]\n",
      "  [55.]\n",
      "  [89.]]]\n",
      "trainY =  [ 34.  21.   8.  55.  89. 144.]\n"
     ]
    }
   ],
   "source": [
    "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
    "    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)    \n",
    "    Y_ind = np.arange(time_steps, len(dat), 1)\n",
    "    Y = dat[Y_ind]\n",
    "    rows_x = len(Y)\n",
    "    X = dat[0:rows_x]\n",
    "    for i in range(time_steps-1):\n",
    "        temp = dat[i+1:rows_x+i+1]\n",
    "        X = np.column_stack((X, temp))\n",
    "        \n",
    "    # random permutation with fixed seed   \n",
    "    rand = np.random.RandomState(seed=13)\n",
    "    idx = rand.permutation(rows_x)\n",
    "    split = int(train_percent*rows_x)\n",
    "    train_ind = idx[0:split]\n",
    "    test_ind = idx[split:]\n",
    "    trainX = X[train_ind]\n",
    "    trainY = Y[train_ind]\n",
    "    testX = X[test_ind]\n",
    "    testY = Y[test_ind]\n",
    "    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))    \n",
    "    testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
    "    return trainX, trainY, testX, testY, scaler\n",
    " \n",
    "trainX, trainY, testX, testY, scaler = get_fib_XY(12, 3, 0.7, False)\n",
    "print('trainX = ', trainX)\n",
    "print('trainY = ', trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d1af26",
   "metadata": {},
   "source": [
    "# Setting Up the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb5e37f",
   "metadata": {},
   "source": [
    "Now let’s set up a small network with two layers. The first one is the SimpleRNN layer, and the second one is the Dense layer. Below is a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97428f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 2)                 8         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set up parameters\n",
    "time_steps = 20\n",
    "hidden_units = 2\n",
    "epochs = 30\n",
    " \n",
    "# Create a traditional RNN network\n",
    "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
    "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    " \n",
    "model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(time_steps,1), \n",
    "                   activation=['tanh', 'tanh'])\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0337f",
   "metadata": {},
   "source": [
    "# Train the Network and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6da67",
   "metadata": {},
   "source": [
    "The next step is to add code that generates a dataset, trains the network, and evaluates it. This time around, we’ll scale the data between 0 and 1. We don’t need to pass the scale_data parameter as its default value is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9866bf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "826/826 - 3s - loss: 9.8271e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 2s - loss: 8.2982e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 2s - loss: 6.8397e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 2s - loss: 5.4779e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 2s - loss: 4.5016e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 2s - loss: 3.5202e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 2s - loss: 2.7377e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 2s - loss: 2.0426e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 2s - loss: 1.4902e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 2s - loss: 1.1409e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 2s - loss: 9.2090e-05 - 2s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 3s - loss: 7.2412e-05 - 3s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 2s - loss: 6.5676e-05 - 2s/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 2s - loss: 6.2180e-05 - 2s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 2s - loss: 4.4382e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 2s - loss: 4.3229e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 2s - loss: 3.9468e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 2s - loss: 3.8444e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 2s - loss: 2.8968e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 2s - loss: 3.2778e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 2s - loss: 2.9121e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 2s - loss: 3.5880e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 2s - loss: 2.1915e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 2s - loss: 2.4781e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 2s - loss: 2.8943e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 2s - loss: 1.9915e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 2s - loss: 2.5017e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 2s - loss: 2.1199e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 2s - loss: 1.8428e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 2s - loss: 2.4194e-05 - 2s/epoch - 2ms/step\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.3780e-05\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0177e-05\n",
      "Train set MSE =  1.3780168956145644e-05\n",
      "Test set MSE =  1.017730392049998e-05\n"
     ]
    }
   ],
   "source": [
    "# Generate the dataset\n",
    "trainX, trainY, testX, testY, scaler  = get_fib_XY(1200, time_steps, 0.7)\n",
    " \n",
    "model_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    " \n",
    " \n",
    "# Evalute model\n",
    "train_mse = model_RNN.evaluate(trainX, trainY)\n",
    "test_mse = model_RNN.evaluate(testX, testY)\n",
    " \n",
    "# Print error\n",
    "print(\"Train set MSE = \", train_mse)\n",
    "print(\"Test set MSE = \", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c72f5",
   "metadata": {},
   "source": [
    "# Adding a Custom Attention Layer to the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf0bed",
   "metadata": {},
   "source": [
    "In Keras, it is easy to create a custom layer that implements attention by subclassing the Layer class. The Keras guide lists clear steps for creating a new layer via subclassing. You’ll use those guidelines here. All the weights and biases corresponding to a single layer are encapsulated by this class. You need to write the __init__ method as well as override the following methods:\n",
    "\n",
    "build(): The Keras guide recommends adding weights in this method once the size of the inputs is known. This method “lazily” creates weights. The built-in function add_weight() can be used to add the weights and biases of the attention layer.\n",
    "call(): The call() method implements the mapping of inputs to outputs. It should implement the forward pass during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405cac5",
   "metadata": {},
   "source": [
    "# The Call Method for the Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b85a1c",
   "metadata": {},
   "source": [
    "The call method of the attention layer has to compute the alignment scores, weights, and context. You can go through the details of these parameters in Stefania’s excellent article on The Attention Mechanism from Scratch. You’ll implement the Bahdanau attention in your call() method.\n",
    "\n",
    "The good thing about inheriting a layer from the Keras Layer class and adding the weights via the add_weights() method is that weights are automatically tuned. Keras does an equivalent of “reverse engineering” of the operations/computations of the call() method and calculates the gradients during training. It is important to specify trainable=True when adding the weights. You can also add a train_step() method to your custom layer and specify your own method for weight training if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0287ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        \n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        \n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1) \n",
    "        \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        \n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        \n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afb93f",
   "metadata": {},
   "source": [
    "# RNN Network with Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afea1db",
   "metadata": {},
   "source": [
    "Let’s now add an attention layer to the RNN network you created earlier. The function create_RNN_with_attention() now specifies an RNN layer, an attention layer, and a Dense layer in the network. Make sure to set return_sequences=True when specifying the SimpleRNN. This will return the output of the hidden units for all the previous time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a102e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 20, 2)             8         \n",
      "                                                                 \n",
      " attention (attention)       (None, 2)                 22        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33\n",
      "Trainable params: 33\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
    "    x=Input(shape=input_shape)\n",
    "    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
    "    attention_layer = attention()(RNN_layer)\n",
    "    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
    "    model=Model(x,outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')    \n",
    "    return model    \n",
    " \n",
    "model_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1, \n",
    "                                  input_shape=(time_steps,1), activation='tanh')\n",
    "model_attention.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a68ac4",
   "metadata": {},
   "source": [
    "# Train and Evaluate the Deep Learning Network with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc1d51",
   "metadata": {},
   "source": [
    "It’s time to train and test your model and see how it performs in predicting the next Fibonacci number of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56e4641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "826/826 - 3s - loss: 0.0014 - 3s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 3s - loss: 0.0013 - 3s/epoch - 4ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 3ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 3s - loss: 0.0012 - 3s/epoch - 3ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 3s - loss: 0.0012 - 3s/epoch - 4ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 3ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 2s - loss: 0.0011 - 2s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 2s - loss: 0.0011 - 2s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 2s - loss: 0.0010 - 2s/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 2s - loss: 9.9590e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 2s - loss: 9.7308e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 2s - loss: 9.3787e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 2s - loss: 8.9168e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 2s - loss: 8.3849e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 2s - loss: 8.0437e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 2s - loss: 7.4899e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 2s - loss: 6.9864e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 2s - loss: 6.5152e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 3s - loss: 6.1190e-04 - 3s/epoch - 3ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 2s - loss: 5.6081e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 2s - loss: 5.1800e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 2s - loss: 4.6997e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 2s - loss: 4.3356e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 2s - loss: 3.8677e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 2s - loss: 3.5211e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 2s - loss: 3.1263e-04 - 2s/epoch - 3ms/step\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 4.3395e-04\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.3711e-04\n",
      "Train set MSE with attention =  0.0004339453880675137\n",
      "Test set MSE with attention =  0.0003371108032297343\n"
     ]
    }
   ],
   "source": [
    "model_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    " \n",
    "# Evalute model\n",
    "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
    "test_mse_attn = model_attention.evaluate(testX, testY)\n",
    " \n",
    "# Print error\n",
    "print(\"Train set MSE with attention = \", train_mse_attn)\n",
    "print(\"Test set MSE with attention = \", test_mse_attn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
